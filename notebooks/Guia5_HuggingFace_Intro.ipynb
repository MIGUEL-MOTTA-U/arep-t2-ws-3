{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad982ac",
   "metadata": {},
   "source": [
    "# Guía — Introducción práctica a Hugging Face (HF)\n",
    "Curso: **IA en el Aula — Nivel Avanzado**  \n",
    "Autor: **Luis Daniel Benavides Navarro**  \n",
    "Fecha: **Octubre 2025**\n",
    "\n",
    "Esta guía introduce el ecosistema de **Hugging Face**: conceptos, librerías principales, opciones de ejecución (local vs remoto) y ejemplos de código en Python. Incluye buenas prácticas, notas de licencias y consideraciones éticas en educación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b28a60",
   "metadata": {},
   "source": [
    "## 1. ¿Qué es Hugging Face?\n",
    "- **Hugging Face (HF)** es un ecosistema abierto para **modelos de aprendizaje automático**, datasets, espacios de demostración y herramientas.\n",
    "- Componentes clave:\n",
    "  - **Hugging Face Hub**: repositorio colaborativo de modelos, datasets y Spaces.\n",
    "  - **Librerías** (desarrolladas por **Hugging Face y su comunidad**):\n",
    "    - **transformers**: modelos de NLP/visión/audio (state-of-the-art) y utilidades de inferencia/entrenamiento.\n",
    "    - **datasets**: carga/limpieza/streaming de conjuntos de datos.\n",
    "    - **tokenizers**: tokenización eficiente (Rust/Python) para modelos modernos.\n",
    "    - **accelerate**, **peft**, **trl**: entrenamiento eficiente, fine-tuning ligero (LoRA), RLHF, etc.\n",
    "  - **Spaces**: apps (Gradio/Streamlit) desplegadas en la nube de HF.\n",
    "\n",
    "**¿Quién desarrolla estas librerías?**  \n",
    "Principalmente el **equipo de ingeniería de Hugging Face** junto con una **amplia comunidad open source** (universidades, empresas y desarrolladores independientes) que contribuyen vía pull requests, issues y discusiones. Los repositorios son públicos (licencias abiertas) y cuentan con mantenedores oficiales de HF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec75e73",
   "metadata": {},
   "source": [
    "## 2. Instalación rápida\n",
    "Ejecuta esta celda para instalar los paquetes base. En entornos gestionados, puedes omitir instalación si ya existen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9874245",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers datasets huggingface_hub tokenizers\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb70c2d",
   "metadata": {},
   "source": [
    "## 3. Conceptos esenciales\n",
    "- **Modelo**: pesos entrenados + arquitectura (`AutoModel*`, `AutoTokenizer`).\n",
    "- **Pipeline**: interfaz de alto nivel para tareas (clasificación, generación, QA, embeddings, etc.).\n",
    "- **Cache local**: HF guarda modelos y datasets en `~/.cache/huggingface/` para reutilización.\n",
    "- **Ejecución local vs remota**:\n",
    "  - *Local*: descargas pesos una vez, ejecutas con tu CPU/GPU.\n",
    "  - *Remota*: usas **Inference API** o **Spaces** (HF los ejecuta en su nube; puede haber límites o costos).\n",
    "- **Model Card**: ficha del modelo (uso previsto, limitaciones, licencias)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0de398b",
   "metadata": {},
   "source": [
    "## 4. Primeros pasos con `transformers` (local)\n",
    "Usaremos `pipeline` para hacer inferencia con modelos ligeros. La primera ejecución descarga los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1315d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Clasificación de sentimientos (modelo ligero por defecto)\n",
    "clf = pipeline(\"sentiment-analysis\")\n",
    "print(clf(\"Este curso de IA en el aula me parece excelente.\"))\n",
    "\n",
    "# Generación de texto (modelo base pequeño)\n",
    "gen = pipeline(\"text-generation\", model=\"gpt2\", max_new_tokens=30)\n",
    "print(gen(\"Hello AI classroom, today we will learn about\", num_return_sequences=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de99ec2",
   "metadata": {},
   "source": [
    "### Notas\n",
    "- La primera ejecución descargará los pesos (conexión requerida) y los guardará en caché.\n",
    "- En ejecuciones posteriores, se cargan desde disco.\n",
    "- Para usar GPU (si existe), pasa `device=0` al crear el pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba625054",
   "metadata": {},
   "source": [
    "## 5. Uso de la API de AutoModel/AutoTokenizer (más control)\n",
    "Cuando necesites más control que `pipeline`, usa las clases automáticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79715852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "inputs = tok(\"I love practical AI courses.\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "pred = torch.softmax(logits, dim=-1).tolist()[0]\n",
    "print({\"NEGATIVE\": pred[0], \"POSITIVE\": pred[1]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bff309",
   "metadata": {},
   "source": [
    "### Explicación detallada: Clasificación de sentimientos con Hugging Face\n",
    "\n",
    "Este ejemplo muestra cómo interactuar directamente con un modelo de Hugging Face sin usar `pipeline`, para comprender el flujo interno de tokenización, inferencia y decodificación.\n",
    "\n",
    "---\n",
    "\n",
    "### Importaciones\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "```\n",
    "\n",
    "- `AutoTokenizer`: descarga el **tokenizador** adecuado al modelo (convierte texto a IDs numéricos).  \n",
    "- `AutoModelForSequenceClassification`: carga el **modelo neuronal** y sus **pesos** preentrenados para tareas de **clasificación** (por ejemplo, sentimiento).  \n",
    "- `torch`: motor numérico (PyTorch) que gestiona tensores y operaciones de la red neuronal.\n",
    "\n",
    "---\n",
    "\n",
    "### Seleccionar el modelo\n",
    "\n",
    "```python\n",
    "model_id = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "```\n",
    "\n",
    "- `distilbert`: versión compacta de **BERT** (Distilled BERT).  \n",
    "- `base-uncased`: vocabulario sin distinción de mayúsculas/minúsculas.  \n",
    "- `finetuned-sst-2`: ajustado sobre el dataset **Stanford Sentiment Treebank v2**, especializado en **análisis de sentimiento (positivo/negativo)**.\n",
    "\n",
    "---\n",
    "\n",
    "### Cargar el tokenizador y el modelo\n",
    "\n",
    "```python\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "```\n",
    "\n",
    "- `from_pretrained()` busca el modelo en la caché local o lo descarga del **Hugging Face Hub**.  \n",
    "- El paquete incluye pesos, configuración, tokenizador y metadatos.  \n",
    "- `tok` traduce texto → números; `model` ejecuta la red neuronal con esos números.\n",
    "\n",
    "---\n",
    "\n",
    "### Tokenizar el texto\n",
    "\n",
    "```python\n",
    "inputs = tok(\"I love practical AI courses.\", return_tensors=\"pt\")\n",
    "```\n",
    "\n",
    "El tokenizador convierte la frase en IDs y máscaras de atención:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'input_ids': tensor([[101, 1045, 2293, 3331, 9935, 4822, 1012, 102]]),\n",
    "    'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "}\n",
    "```\n",
    "\n",
    "- `input_ids`: IDs numéricos correspondientes a las subpalabras.  \n",
    "- `attention_mask`: marca con 1 las posiciones activas y con 0 el relleno (padding).  \n",
    "- `return_tensors=\"pt\"` devuelve tensores de PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "### Inferencia (paso hacia adelante del modelo)\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "```\n",
    "\n",
    "- `torch.no_grad()`: desactiva el cálculo de gradientes (modo inferencia, más eficiente).  \n",
    "- `model(**inputs)`: pasa los tensores por las capas de la red neuronal.  \n",
    "- `.logits`: salida sin normalizar, vector con una puntuación por clase.  \n",
    "\n",
    "Ejemplo:\n",
    "```python\n",
    "tensor([[-2.13, 3.56]])\n",
    "```\n",
    "→ Puntuación baja para *NEGATIVE*, alta para *POSITIVE*.\n",
    "\n",
    "---\n",
    "\n",
    "### Convertir logits en probabilidades\n",
    "\n",
    "```python\n",
    "pred = torch.softmax(logits, dim=-1).tolist()[0]\n",
    "```\n",
    "\n",
    "- `softmax`: convierte las puntuaciones en probabilidades que suman 1.  \n",
    "- `dim=-1`: aplica la operación sobre la última dimensión (las clases).  \n",
    "- `tolist()[0]`: transforma el tensor a una lista Python.\n",
    "\n",
    "Ejemplo:\n",
    "```python\n",
    "[0.01, 0.99]\n",
    "```\n",
    "→ 1% negativo, 99% positivo.\n",
    "\n",
    "---\n",
    "\n",
    "### Mostrar el resultado\n",
    "\n",
    "```python\n",
    "print({\"NEGATIVE\": pred[0], \"POSITIVE\": pred[1]})\n",
    "```\n",
    "Salida:\n",
    "```python\n",
    "{'NEGATIVE': 0.0123, 'POSITIVE': 0.9877}\n",
    "```\n",
    "El modelo predice un sentimiento **claramente positivo** para la frase.\n",
    "\n",
    "---\n",
    "\n",
    "### Resumen del flujo interno\n",
    "\n",
    "| Etapa | Qué hace | Tipo de dato |\n",
    "|--------|-----------|--------------|\n",
    "| Tokenizer | Convierte texto → IDs | Diccionario de tensores |\n",
    "| Modelo | Procesa los tensores y produce logits | Tensor 2D |\n",
    "| Softmax | Convierte logits → probabilidades | Lista o array |\n",
    "| Salida final | Devuelve un diccionario con clases y probabilidades | Dict |\n",
    "\n",
    "---\n",
    "\n",
    "### Conceptos clave para el aula\n",
    "\n",
    "- **Tokenización:** convierte lenguaje humano a números comprensibles por el modelo.  \n",
    "- **Logits vs Probabilidades:** logits son puntuaciones sin escalar; `softmax` las convierte en probabilidades.  \n",
    "- **Fine-tuning:** ajuste de un modelo base a una tarea específica.  \n",
    "- **Inferencia:** uso del modelo para predecir (sin entrenamiento).  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5905a99",
   "metadata": {},
   "source": [
    "## 6. Trabajar con `datasets`\n",
    "La librería `datasets` permite cargar datos públicos del Hub y tratarlos como DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b78ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Carga un dataset de ejemplo (pequeño)\n",
    "ds = load_dataset(\"ag_news\", split=\"train[:1000]\")\n",
    "print(ds)\n",
    "print(ds[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b2ff0",
   "metadata": {},
   "source": [
    "## 7. Hugging Face Inference API (remoto)\n",
    "Si no quieres descargar ni ejecutar modelos localmente, puedes llamar a la **Inference API**. Requiere una cuenta HF y, en algunos casos, **token de acceso** (con cuota gratuita limitada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd6075",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "\n",
    "# Opcional: configura tu token HF si es necesario\n",
    "# os.environ['HF_TOKEN'] = 'hf_...'\n",
    "\n",
    "client = InferenceClient(model=\"gpt2\")  # modelo público sencillo\n",
    "out = client.text_generation(\"Hello from remote HF Inference API!\", max_new_tokens=32)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf88e0a0",
   "metadata": {},
   "source": [
    "### ¿Cuándo usar local vs remoto?\n",
    "- **Local**: control total, sin costos por uso; requiere recursos (CPU/GPU/RAM) y descarga de pesos.\n",
    "- **Remoto (Inference API/Spaces)**: cero instalación, útil para demos/producción; puede tener límites/costos.\n",
    "- En docencia: comenzar local con modelos pequeños; escalar a remoto si es necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605bfd94",
   "metadata": {},
   "source": [
    "## 8. Embeddings (representaciones vectoriales)\n",
    "Los embeddings son útiles en **búsqueda semántica**, **RAG** y **análisis de similitud**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3d1ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "emb_model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "emb_tok = AutoTokenizer.from_pretrained(emb_model_id)\n",
    "emb_model = AutoModel.from_pretrained(emb_model_id)\n",
    "\n",
    "def embed(texts):\n",
    "    # Tokenización con padding/truncado\n",
    "    batch = emb_tok(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        out = emb_model(**batch)\n",
    "    # Mean pooling simple sobre la última capa\n",
    "    tokens = out.last_hidden_state  # [batch, seq, hidden]\n",
    "    mask = batch[\"attention_mask\"].unsqueeze(-1)  # [batch, seq, 1]\n",
    "    masked = tokens * mask\n",
    "    sent_emb = masked.sum(dim=1) / mask.sum(dim=1)\n",
    "    return F.normalize(sent_emb, p=2, dim=1)\n",
    "\n",
    "e = embed([\"inteligencia artificial en educación\", \"clase de programación\", \"oxigenación en hidroeléctricas\"])\n",
    "print(e.shape)\n",
    "# Similitud coseno entre primera y segunda\n",
    "sim = (e[0] @ e[1]).item()\n",
    "print(\"Cosine similarity (0 vs 1):\", round(sim, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae06be84",
   "metadata": {},
   "source": [
    "# Comprendiendo los Embeddings en Inteligencia Artificial\n",
    "\n",
    "## ¿Qué es un *embedding*?\n",
    "\n",
    "Un **embedding** es una **representación numérica** de un dato (texto, imagen, audio, etc.) en un **espacio vectorial continuo**.  \n",
    "En lugar de trabajar directamente con palabras o símbolos, los modelos de IA los transforman en **vectores de números reales**, de modo que conceptos similares queden **cercanos entre sí** en ese espacio.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "| Palabra | Embedding (simplificado) |\n",
    "|----------|--------------------------|\n",
    "| \"perro\" | [0.82, 0.10, 0.44, …] |\n",
    "| \"gato\"  | [0.80, 0.12, 0.46, …] |\n",
    "| \"avión\" | [-0.30, 0.90, -0.12, …] |\n",
    "\n",
    "Si calculamos la distancia entre vectores, veremos que **“perro”** y **“gato”** están mucho más cerca que **“perro”** y **“avión”**.  \n",
    "Esto significa que el modelo **captura relaciones semánticas** (de significado) entre palabras.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuición geométrica\n",
    "\n",
    "Los embeddings convierten el lenguaje en **puntos en un espacio N-dimensional**, donde la geometría refleja las relaciones conceptuales:\n",
    "\n",
    "- Distancias pequeñas → conceptos similares.  \n",
    "- Distancias grandes → conceptos distintos.  \n",
    "- A veces incluso se pueden representar relaciones lineales:\n",
    "  ```\n",
    "  vector(\"rey\") - vector(\"hombre\") + vector(\"mujer\") ≈ vector(\"reina\")\n",
    "  ```\n",
    "\n",
    "Por eso los embeddings son la base de muchas tareas de **razonamiento semántico** en IA.\n",
    "\n",
    "---\n",
    "\n",
    "## Cómo se generan los embeddings\n",
    "\n",
    "Los embeddings se **aprenden** durante el entrenamiento de modelos.  \n",
    "En redes neuronales, la primera capa suele ser una **capa de embedding** que asigna a cada palabra un vector.\n",
    "\n",
    "- En modelos clásicos como **Word2Vec**, **GloVe** o **FastText**, los embeddings se entrenan explícitamente observando qué palabras aparecen juntas.\n",
    "- En modelos modernos (**BERT**, **GPT**, **DistilBERT**), los embeddings son el resultado de las **capas internas** de atención.  \n",
    "  Se pueden extraer desde cualquiera de esas capas usando librerías como `transformers`.\n",
    "\n",
    "---\n",
    "\n",
    "## Embeddings en Hugging Face\n",
    "\n",
    "Podemos usar un modelo especializado en *sentence embeddings* (como los de **Sentence Transformers**) para convertir textos completos en vectores comparables:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "\n",
    "def embed(texts):\n",
    "    batch = tok(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        out = model(**batch)\n",
    "    # Promedio (mean pooling)\n",
    "    tokens = out.last_hidden_state\n",
    "    mask = batch[\"attention_mask\"].unsqueeze(-1)\n",
    "    sent_emb = (tokens * mask).sum(1) / mask.sum(1)\n",
    "    return F.normalize(sent_emb, p=2, dim=1)\n",
    "\n",
    "emb = embed([\"inteligencia artificial\", \"aprendizaje automático\", \"física cuántica\"])\n",
    "print(emb.shape)\n",
    "```\n",
    "\n",
    "Cada texto se convierte en un vector de dimensión 384 o 768 (según el modelo).\n",
    "\n",
    "---\n",
    "\n",
    "## Comparación entre embeddings\n",
    "\n",
    "Para comparar embeddings se usa la **similitud del coseno**, que mide el ángulo entre los vectores:\n",
    "\n",
    "\\[\n",
    "\\text{similaridad}(A,B) = \\frac{A \\cdot B}{||A|| \\, ||B||}\n",
    "\\]\n",
    "\n",
    "Valores:\n",
    "- 1.0 → textos muy similares.  \n",
    "- 0.0 → no relacionados.  \n",
    "- -1.0 → opuestos conceptualmente.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "```python\n",
    "sim = float((emb[0] @ emb[1]).item())\n",
    "print(\"Similitud coseno:\", round(sim, 4))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Aplicaciones educativas y prácticas\n",
    "\n",
    "Los embeddings son fundamentales en muchos sistemas modernos:\n",
    "\n",
    "| Aplicación | Uso de embeddings |\n",
    "|-------------|------------------|\n",
    "| **RAG (Retrieval-Augmented Generation)** | Recuperar documentos relevantes antes de generar texto. |\n",
    "| **Búsqueda semántica** | Encontrar textos “parecidos” por significado, no por palabras exactas. |\n",
    "| **Clasificación y clustering** | Agrupar frases o estudiantes por temas o patrones. |\n",
    "| **Recomendadores** | Calcular similitud entre recursos educativos. |\n",
    "| **Análisis de discurso** | Detectar similitudes en respuestas escritas o redacciones. |\n",
    "\n",
    "---\n",
    "\n",
    "## Buenas prácticas\n",
    "\n",
    "- Normaliza los vectores antes de compararlos (`F.normalize` o división por norma).  \n",
    "- Usa embeddings de **oraciones** (Sentence Transformers) para comparar frases completas.  \n",
    "- Almacena embeddings en bases vectoriales como **FAISS** o **Chroma** para búsquedas rápidas.  \n",
    "- Recuerda que los embeddings reflejan el **sesgo** del modelo con el que se entrenaron; úsalo de forma crítica y responsable.\n",
    "\n",
    "---\n",
    "\n",
    "## En resumen\n",
    "\n",
    "> Un *embedding* es una forma matemática de representar significado.  \n",
    "> Convierte información simbólica (palabras, imágenes, sonidos) en **vectores** donde la distancia representa **relación semántica**.  \n",
    "> Son el puente entre el lenguaje humano y el razonamiento numérico de los modelos de IA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf2d8e1",
   "metadata": {},
   "source": [
    "## 9. Tokenización con `tokenizers`\n",
    "`tokenizers` (Rust + Python) es la base de la tokenización rápida y reproducible para modelos modernos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085a68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "# Ejemplo mínimo: crear un tokenizer vacío BPE (demostrativo)\n",
    "tokenizer = Tokenizer(BPE())\n",
    "# Nota: entrenar un tokenizer real requiere un corpus y procesos adicionales (no cubierto aquí).\n",
    "print(\"Tokenizer BPE de ejemplo creado (demo).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826326e4",
   "metadata": {},
   "source": [
    "## 10. Buenas prácticas, licencias y ética\n",
    "- Revisa la **Model Card** antes de usar un modelo: usos previstos, sesgos, limitaciones.\n",
    "- Respeta las **licencias** de modelos/datasets; algunos son comerciales, otros sólo para investigación.\n",
    "- En el aula: evita datos personales reales, valida salidas y cita fuentes de modelo/dataset.\n",
    "- Optimiza recursos: usa modelos pequeños para clases; documenta versiones y hashes de commit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0a344",
   "metadata": {},
   "source": [
    "## 11. Solución de problemas comunes\n",
    "- **`ModuleNotFoundError`**: reinstala el paquete faltante; reinicia kernel.\n",
    "- **`OSError: Can't load tokenizer/model`**: el ID del modelo es incorrecto o no tienes permisos.\n",
    "- **Falta de RAM/GPU**: usa modelos más pequeños o recurre a Inference API.\n",
    "- **Rate limit (remoto)**: espera o considera un plan de pago.\n",
    "- **Conectividad**: comprueba proxies/firewalls y credenciales (HF_TOKEN si aplica)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219454f3",
   "metadata": {},
   "source": [
    "## 12. Próximos pasos (sugerencias)\n",
    "- Fine-tuning ligero con **PEFT/LoRA** sobre un dataset pequeño.\n",
    "- Construir un mini-**RAG** con embeddings + FAISS/Chroma.\n",
    "- Publicar una demo en **HF Spaces** (Gradio) para compartir con estudiantes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
