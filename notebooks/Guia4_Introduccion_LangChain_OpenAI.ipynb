{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83152e03",
   "metadata": {},
   "source": [
    "# Guía 3 — Introducción a LangChain con OpenAI API\n",
    "Curso: **IA en el Aula — Nivel Avanzado**  \n",
    "Profesor: **Luis Daniel Benavides Navarro**  \n",
    "Fecha: **Octubre 2025**\n",
    "\n",
    "En esta guía aprenderás los conceptos básicos de **LangChain**, una librería diseñada para construir aplicaciones impulsadas por modelos de lenguaje, integrando la API de OpenAI con flujos más complejos. Exploraremos cómo crear un primer **prompt chain**, administrar memoria y usar herramientas para componer respuestas inteligentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4929bd",
   "metadata": {},
   "source": [
    "## 1️⃣ Instalación y configuración inicial\n",
    "LangChain se instala como cualquier otra librería de Python. También usaremos `python-dotenv` para gestionar la clave de OpenAI. Ejecute la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca0e6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai langchain python-dotenv langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aee39b5",
   "metadata": {},
   "source": [
    "## 2️⃣ Cargar variables de entorno y cliente OpenAI\n",
    "Crea un archivo `.env` con tu clave de API y cárgala para poder usarla dentro de LangChain. Esto evita exponer tu clave en el código fuente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3fcb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()  # Cargar archivo .env\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"No se encontró la clave OPENAI_API_KEY en el archivo .env\")\n",
    "\n",
    "# Crear cliente LangChain con OpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "print(\"Cliente LangChain con OpenAI inicializado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e2926d",
   "metadata": {},
   "source": [
    "## 3️⃣ Primer ejemplo: Prompt simple con LangChain\n",
    "LangChain utiliza **chains** (cadenas de pasos) para procesar información. Comencemos con una cadena simple que envía un mensaje al modelo y obtiene la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0769dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the LLM (uses your OpenAI API key from environment)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "\n",
    "# Create a simple prompt\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explica en dos frases el concepto de {tema}.\"\n",
    ")\n",
    "\n",
    "# Combine the components using LCEL (LangChain Expression Language)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run it\n",
    "result = chain.invoke({\"tema\": \"aprendizaje automático\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5349f8b",
   "metadata": {},
   "source": [
    "### Explicación\n",
    "- **ChatPromptTemplate:** define la estructura del mensaje que se envía al modelo.\n",
    "- **ChatOpenAI:** la conexión real al modelo.\n",
    "- **chain** crea la cadena de componentes usando LCEL (LangChain Expression Language).\n",
    "- **StrOutputParser** convierte la salida estructurada del modelo en texto plano.\n",
    "- **chain.invoke:** ejecuta la cadena pasando los valores del prompt.\n",
    "\n",
    "Este patrón permite reutilizar prompts para distintos temas o contextos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6939352f",
   "metadata": {},
   "source": [
    "## 4️⃣ Ejemplo 2: Encadenar múltiples pasos\n",
    "LangChain permite combinar varios pasos en una misma ejecución. En este ejemplo, crearemos dos prompts: uno para definir un tema y otro para generar una aplicación educativa basada en el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c8d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# Primer paso: obtener una descripción\n",
    "primer_prompt = PromptTemplate(\n",
    "    input_variables=[\"tema\"],\n",
    "    template=\"Explica brevemente el concepto de {tema}.\"\n",
    ")\n",
    "primer_chain = LLMChain(llm=llm, prompt=primer_prompt)\n",
    "\n",
    "# Segundo paso: generar aplicación educativa\n",
    "segundo_prompt = PromptTemplate(\n",
    "    input_variables=[\"concepto\"],\n",
    "    template=\"Propón una aplicación educativa del siguiente concepto: {concepto}.\"\n",
    ")\n",
    "segundo_chain = LLMChain(llm=llm, prompt=segundo_prompt)\n",
    "\n",
    "# Combinar ambos pasos en una cadena secuencial\n",
    "chain_secuencial = SimpleSequentialChain(chains=[primer_chain, segundo_chain])\n",
    "\n",
    "# Ejecutar la cadena completa\n",
    "resultado = chain_secuencial.run(\"realidad aumentada\")\n",
    "print(resultado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd03c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "# 1) LLM (usa tu OPENAI_API_KEY en el entorno)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "to_str = StrOutputParser()\n",
    "\n",
    "# 2) Paso 1: explicar brevemente el concepto de {tema}\n",
    "primer_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explica brevemente el concepto de {tema}.\"\n",
    ")\n",
    "primer_paso = primer_prompt | llm | to_str\n",
    "# `primer_paso` produce un string, por ejemplo: \"La realidad aumentada es ...\"\n",
    "\n",
    "# 3) Paso 2: proponer una aplicación educativa usando la salida del paso 1 como {concepto}\n",
    "segundo_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Propón una aplicación educativa del siguiente concepto: {concepto}.\"\n",
    ")\n",
    "segundo_paso = segundo_prompt | llm | to_str\n",
    "\n",
    "# 4) Encadenar: mapear la entrada {tema} al primer paso, y su salida a {concepto} del segundo\n",
    "cadena_secuencial = {\"concepto\": primer_paso} | segundo_paso\n",
    "\n",
    "# 5) Ejecutar la cadena completa\n",
    "resultado = cadena_secuencial.invoke({\"tema\": \"realidad aumentada\"})\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf949db6",
   "metadata": {},
   "source": [
    "### Explicación\n",
    "- **cadena_secuencial:** permite conectar varias cadenas; la salida de una se convierte en la entrada de la siguiente.\n",
    "- En este caso, el modelo primero explica el tema y luego sugiere una aplicación educativa.\n",
    "\n",
    "Este tipo de flujo es ideal para generar contenido didáctico o ideas para proyectos en clase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0483d0",
   "metadata": {},
   "source": [
    "## 5️⃣ Ejemplo 3: Añadir memoria a la conversación\n",
    "LangChain incluye módulos de **memoria** para mantener contexto entre múltiples interacciones. Esto permite simular conversaciones educativas más naturales, donde el modelo recuerda temas previos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d363f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instalar si hace falta:\n",
    "# %pip install -U langchain langchain-openai\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM (requiere OPENAI_API_KEY en el entorno)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "to_str = StrOutputParser()\n",
    "\n",
    "# Prompt con hueco para el historial\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente educativo claro y conciso.\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),      # ← aquí va la memoria\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Cadena base\n",
    "chain = prompt | llm | to_str\n",
    "\n",
    "# Memoria simple como lista de mensajes\n",
    "history: list = []\n",
    "\n",
    "def chat(user_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Envía un turno del usuario, usa el historial y actualiza la memoria\n",
    "    con el par (usuario, asistente).\n",
    "    \"\"\"\n",
    "    global history\n",
    "    # Ejecutar la cadena inyectando el historial actual\n",
    "    answer = chain.invoke({\"input\": user_text, \"chat_history\": history})\n",
    "    # Actualizar memoria (guardar los dos mensajes)\n",
    "    history += [HumanMessage(content=user_text), AIMessage(content=answer)]\n",
    "    return answer\n",
    "\n",
    "# --- Ejemplo de uso (tres turnos) ---\n",
    "print(chat(\"Hola, soy un profesor de informática.\"))\n",
    "print(chat(\"¿Puedes explicarme cómo introducir IA a mis estudiantes?\"))\n",
    "print(chat(\"¿Qué ejemplos prácticos puedo usar en la clase?\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a79423b",
   "metadata": {},
   "source": [
    "### Explicación\n",
    "- **ConversationBufferMemory:** almacena los mensajes anteriores en la conversación.\n",
    "- **ConversationChain:** combina el modelo con la memoria.\n",
    "- Esta funcionalidad es útil para tutores inteligentes o chatbots educativos que requieren continuidad en el diálogo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ba0ab9",
   "metadata": {},
   "source": [
    "## 6️⃣ Buenas prácticas con LangChain + OpenAI\n",
    "- Usa prompts **claros y estructurados**: el modelo responde mejor cuando la tarea está bien definida.\n",
    "- Controla `temperature` según la tarea: bajo (0.1–0.3) para precisión, alto (0.7–0.9) para creatividad.\n",
    "- Guarda logs o historiales si tu aplicación incluye interacción prolongada.\n",
    "- Limita la longitud de las respuestas con `max_tokens` para evitar costos o respuestas excesivas.\n",
    "- Documenta tus cadenas (`LLMChain`) para reusarlas en distintos contextos educativos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a40d49",
   "metadata": {},
   "source": [
    "## ✅ Conclusión\n",
    "Has aprendido los fundamentos de LangChain: prompts, chains, memoria y flujo secuencial. Estos conceptos son la base para desarrollar asistentes educativos, tutores personalizados o sistemas de generación de contenido en el aula. En la próxima guía implementaremos un **asistente educativo con RAG (Retrieval-Augmented Generation)** usando tus propios materiales docentes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
